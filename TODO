# -*- org -*-
* TODO Oh, shit: and don't forget about parallelization.
  The regex-based, filtering guessing-strategy is susceptible to
  parallelization; isn't it? See [[http://stackoverflow.com/questions/2622750/why-does-clojure-hang-after-having-performed-my-calculations][this]], though:

  #+BEGIN_QUOTE
  About pfilter, it should work but run slower than filter, since your
  predicate is simple. Parallelization isn't free so you have to give
  each thread moderately intensive tasks to offset the multithreading
  overhead. Batch your items before filtering them.
  #+END_QUOTE
* TODO Trie based guessing strategy
  Generate trie at startup; break links as a filtering mechanism (have
  to copy the datastructure, therefore). Trie of cardinality 26
  corresponding to $a \to z$; normally used for prefix matching. Can
  we really beat (the more general) regex by iteration?

  See Bagwell's [[http://lampwww.epfl.ch/papers/idealhashtrees.pdf][Ideal Hash Tries]], by the way.

  An initial implementation could use nested =hash-sets= or
  =sorted-sets=; why not arrays of cardinality 26, though? They might
  be sparse, and we have to do key-iteration on unknown letters. Let's
  compare hash-set-, sorted-set-, hash-map-, sorted-map- and
  vector-based implementations. Let's start with vector.

  How do we reject candidates: sentinel at leaves? Segregate tries by
  cardinality and check depth?

  That's right, though: tries need to be of cardinality 26 + 1 to
  account for sentinels.

  Let's do hash-tables; they're more general, though. Otherwise: we
  have to do the character -> index mapping (which is a form of
  hashing, by the way).

  We need: =insert= and =search=; let's do this utterly ad-hoc to
  strings; such that we can insert and search for strings. Let's even
  formalize the =_=-as-wildcard mechanism.

  Question between vectors and hash-tables comes down to whether
  linear search over the vector-cardinality is longer than
  key-retrieval for the latter (I suspect that it is, by the way, in
  addition to being a pain-in-the-ass).

  And this all before we've done any profiling; hmm. =jvisualvm=
  didn't yield anything interesting (that I could determine). Manual
  profiling, on the other hand, revealed that the application of =merge=
  over map-dyads is expense; reduction over =assoc= yielded ten
  percent savings.

  Let's do a stack-based traversal of the tree (depth-first; I wonder:
  will depth- or breadth-first make a difference?); a wild-card adds
  all letters to the traversal stack. As soon as we encounter a letter
  that doesn't exist, we can sever the tie; can't we?

# <<node-histography>>

  How are we going to do the letter->frequency histogram with a trie,
  by the way? Doesn't it require a complete trie traversal? This
  requires memory overhead, of course: but if each node in the trie
  stored histography going down, we could delete the node and update
  the histography immediately.

  The storage required is probably pretty big, though: a 26-arity
  vector for every vector. Or is there some kind of Huffman encoding
  we could do? It is essentially a 26-ary Huffman encoding. (See
  [[http://en.wikipedia.org/wiki/Huffman_coding#n-ary_Huffman_coding][n-ary Huffman coding]], btw.

  Priority queue. Each node is an object containing: the next vector
  down; a vector of frequencies for each letter down. There's got to
  be a clever algorithm for this. How fast, though; and do we have
  egregious initialization times?
** DONE Is comparing chars faster than comparing ints?
   CLOSED: [2011-08-23 Tue 21:06]
   - CLOSING NOTE [2011-08-23 Tue 21:06] \\
     It turns out they're roughly the same (roughly 500 msecs).
   #+BEGIN_SRC clojure
     (time (reduce = (replicate 1000000 \a)))
     (time (reduce = (replicate 1000000 1)))
   #+END_SRC
** DONE Test that mutating shallow clones of HashMaps does not mutate clonee.
   CLOSED: [2011-08-23 Tue 21:08]
   - CLOSING NOTE [2011-08-23 Tue 21:08] \\
     It is indeed the case.
   #+BEGIN_SRC clojure
     (let [hash-map (new java.util.HashMap)]
       (. hash-map put "a" 1)
       (. hash-map put "b" 2)
       (let [hash-map-clone (. hash-map clone)]
         (. hash-map-clone remove "a")
         (assert (= {"a" 1 "b" 2} (into {} hash-map)))))
   #+END_SRC
** TODO Trie from word
   #+BEGIN_SRC clojure
     ;;; Can we keep them as transients? We have to copy and mutate them
     ;;; later on, though.
     
     ;;; The weird thing about this is that it ends on an empty hashmap as
     ;;; a form of sentinel.
     (let [trie (new java.util.HashMap)
           word "harro"]
       (reduce (fn [trie letter]
                 ;; Ugly; what's the idiom: chaining? if-let?
                 (let [subtrie (. trie get letter)
                       subtrie (if subtrie subtrie (new java.util.HashMap))]
                   (. trie put letter subtrie)
                   subtrie))
               trie
               word)
       trie)
     
   #+END_SRC

   We decided in the car to build the trie up from the leaves using a
   forest of letters indexed by a vector; build it back up to root
   (could also go the other way), adding things along the way. This is
   arbitrary, since we're not doing prefix-stems here.

   We're also going to deal with vectors of integers, not hashmaps of
   chars; though the latter is a space-saving hack.

   #+BEGIN_SRC clojure
     (use 'clojure.contrib.trace)
     
     (reduce (fn [trie letter]
               (let [subtrie (trie letter {})]
                 (assoc trie letter subtrie)
                 subtrie))
             {}
             "harro")
     
     (defn word->trie [trie word]
       (loop [word word
              trie trie]
         (let [letter (first word)]
           (if letter
             (recur (next word)
                    (assoc trie letter (trie letter {})))
             trie))))
     
     (defn make-trie []
       (transient (vec (repeat 26 nil))))
     
     ;; (trace make-trie)
     
     ;;; We can't iterate over the transient tree without persisting it;
     ;;; but we can't mutate the tree once we've persisted it. Lazy
     ;;; persistent tries?
     (defn persistent-trie! [trie]
       (if trie
         (let [trie (persistent! trie)]
           (doseq [subtrie trie]
             (if subtrie
               (persistent-trie! subtrie)))
           trie)))
     
     (defn persistent-trie! [trie]
       (if trie
         (do
           (let [n (count trie)]
             (loop [i 0]
               (if (< i n)
                 (let [subtrie (trie i)]
                   (if subtrie
                     (do
                       (persistent-trie! subtrie)
                       (assoc! trie i (persistent! subtrie))))
                   (recur (inc i)))))
             trie))))
     
     (defn hash-word! [trie word]
       (reduce (fn [trie letter]
                 (let [subtrie (trie letter)
                       subtrie (if subtrie subtrie (make-trie))]
                   (assoc! trie letter subtrie)
                   subtrie))
               trie
               word))
     
     (trace hash-word!)
     
     (let [words (take 200000 (repeatedly (fn [] (take 7 (repeatedly (fn [] (rand-int 26)))))))
           trie (make-trie)]
       (doseq [word words] (hash-word! trie word))
       (persistent! (persistent-trie! trie)))
     
     #_ (let [trie (make-trie)]
       (hash-word! trie [8 24 3 23 12 20 15])
       (persistent! (persistent-trie! trie)))
     
   #+END_SRC

   To do this bottom-up approach, we're going to operate on words of
   the same arity. We can't do that, though: not all leaves of value n
   are the same because of the prefix-context. Can we count somehow
   from the root down?

   As opposed to packing a letter/histogram in the same node, let's
   have two orthogonal tries: frequency-trie and letter-trie.
** TODO Trie from dictionary
   We could trie every word and do a merge-reduction on the root to
   avoid mutation; expensive? Also, I'd like to keep track of
   [[node-histography]].

   Crafting this 

   #+BEGIN_SRC clojure
     (use '[clojure.contrib.io :only (reader)])
     
     ;;; We're not going to be able to transient this, because all subtries
     ;;; will have to be transient and persisted, too; unless we do a
     ;;; nested persistence.
     (defn make-trie-dictionary [file]
       (with-open [input (reader file)]
         (binding [*in* input]
           (let [trie (transient {})]
             (loop [word (read-line)]
               (if word
                 (do
                   
                   (recur
                    (read-line)))
                 (persistent! trie)))))))
     
     (make-trie-dictionary "words.txt")
     
   #+END_SRC
* TODO We are assuming that the dictionary is lower-case.
* TODO Check TODO.pdf into the repository as documentation?
  It goes against checking derivative things in; maybe we can generate
  it for releases. Here's a [[http://stackoverflow.com/questions/1431082/programmatically-add-files-to-a-github-download-page][half-ass discussion]]; 'tis true, though,
  that PDF is a PS-derivative and, as such, is not wholly binary.

  We can .gitignore =TODO.pdf=, such that we only check it in "when we
  really mean it."
* DONE `run' should return the score and total time.
  CLOSED: [2011-08-22 Mon 20:13]
  - CLOSING NOTE [2011-08-22 Mon 20:13] \\
    The `time-not-value' macro; cf. `[[https://github.com/clojure/clojure/blob/f86db9cc68773dd3e4a166c1ff7b81e4a98aa602/src/clj/clojure/core.clj#L2959][time]]'.
  Are we going to have to parse the output of =time=, somehow? Or,
  just coöpt the [[http://stackoverflow.com/questions/3041299/how-to-benchmark-functions-in-clojure][time]] macro.

  #+BEGIN_SRC clojure
    (defmacro time-not-value
      [expr]
      `(let [start# (. System (nanoTime))]
         ~expr
         (/ (double (- (. System (nanoTime)) start#)) 1000000.0)))
    
  #+END_SRC
* DONE Maps
  CLOSED: [2011-08-22 Mon 14:47]
  #+BEGIN_QUOTE
  A Map is a collection that maps keys to values. Two different map
  types are provided - hashed and sorted. Hash maps require keys that
  correctly support hashCode and equals. Sorted maps require keys that
  implement Comparable, or an instance of Comparator. Hash maps
  provide faster access (log32N hops) vs (logN hops), but sorted maps
  are, well, sorted. count is O(1). conj expects another (possibly
  single entry) map as the item, and returns a new map which is the
  old map plus the entries from the new, which may overwrite entries
  of the old. conj also accepts a MapEntry or a vector of two items
  (key and value). seq returns a sequence of map entries, which are
  key/value pairs. Sorted map also supports rseq, which returns the
  entries in reverse order. Maps implement IFn, for invoke() of one
  argument (a key) with an optional second argument (a default value),
  i.e. maps are functions of their keys. nil keys and values are ok.

  Related functions
  - Create a new map :: hash-map sorted-map sorted-map-by
  - 'Change' a map :: assoc dissoc select-keys merge merge-with zipmap
  - Examine a map :: get contains? find keys vals map?
  - Examine a map entry :: key val
  #+END_QUOTE
* DONE Regex then trie
  CLOSED: [2011-08-22 Mon 14:48]
# <<regex-then-tries>>
  Let's work with a regex solution that treats the words holistically
  (i.e. as strings); if we can beat regex with some sort of trie:
  great.
* DONE Does the sorted-map-by comparator have access to the values as well?
  CLOSED: [2011-08-19 Fri 02:41]
  - CLOSING NOTE [2011-08-19 Fri 02:41] \\
    Apparently not
  #+BEGIN_SRC clojure
    (assert (= "3 2"
               (with-out-str
                 (sorted-map-by
                  (fn [x y] (print (format "%s %s" x y)) true) 2 \a 3 \b))))
  #+END_SRC
* DONE How do maps behave with key-collision?
  CLOSED: [2011-08-19 Fri 02:41]
  - CLOSING NOTE [2011-08-19 Fri 02:41] \\
    Replacement
  #+BEGIN_SRC clojure
    (assert (= {2 4} (assoc (assoc (hash-map) 2 3) 2 4)))
    (assert (= {2 4} (assoc (assoc {} 2 3) 2 4)))
  #+END_SRC
* DONE Extract some game mechanics into hangman.core.
  CLOSED: [2011-08-18 Thu 03:56]
* DONE Regex-based GuessingStrategy
  CLOSED: [2011-08-22 Mon 14:47]
  (New pseudo-code syntax, by the way: parenthesized phrases are
  potential optizimations; bracketed phrases are commentary. Also:
  TODOs are full sentences with full stop.)

  #+BEGIN_SRC org
    ,- Create dictionary.
    ,  - Read file and create a list of words.
    ,    - (Order the list by arity, so that pre-filtering is more
    ,      efficient.)
    ,- Initialize filter-strategy on HangmanGame and dictionary.
    ,  - Copy the dictionary.
    ,    - [We're going to have state, unfortunately, since we don't
    ,      control the game loop; or can we use continuations, =yield= or
    ,      coroutines?
    ,  - (Pre-filter dictionary by letter-arity.)
    ,- Next guess
    ,  - Convert =game.getGuessedSoFar= into a suitable regex.
    ,    - Create negative regex: =String.format("[^%s]", new
    ,      String(game.getIncorrectlyGuessedLetters()))=
    ,    - Substitution on =game.getGuessedSoFar= of
    ,      =HangmanGame.MYSTERY_LETTER= for negative regex.
    ,    - Prepend =^=, append =$=.
    ,  - Destructively filter dictionary on regex.
    ,  - If solution is unambiguous, guess word.
    ,    - (Alternatively, some sort of score based on number of possibly
    ,      solutions and number of remaining guesses.)
    ,    - Otherwise, create a sorted map of frequency -> letter (counting
    ,      one letter per word); guess one random sample from the maximally
    ,      frequent letters.
  #+END_SRC
  
  #+BEGIN_SRC clojure	
    (use '[clojure.contrib.io :only (reader)]
         '[clojure.contrib.string :only (replace-str)]
         'hangman.core
         '[clojure.set :only (difference map-invert)]
         '[clojure.contrib.math :only (abs)]
         '[clojure.pprint :only (pprint)]
         'clojure.contrib.profile)
    
    (import '(com.factual.hangman
              GuessingStrategy
              GuessLetter
              GuessWord
              HangmanGame
              HangmanGame$Status)
            'java.lang.Character)
    
    ;;; Possible optimization: sort by arity.
    (defn make-dictionary [dictionary-file]
      (with-open [dictionary-input (reader dictionary-file)]
        (binding [*in* dictionary-input]
          ;; I would use read-lines here, but I've run into stack
          ;; overflows.
          (loop [word (read-line) words '()]
            (if word
              (recur (read-line) (cons word words))
              words)))))
    
    (defn make-arity->dictionary [dictionary-file]
      (with-open [dictionary-input (reader dictionary-file)]
        (binding [*in* dictionary-input]
          (loop [word (read-line)
                 arity->dictionary {}]
            (if word
              (let [arity (count word)]
                (recur (read-line)
                       (assoc arity->dictionary arity (cons word (arity->dictionary arity '())))))
              arity->dictionary)))))
    
    (defn random-letter []
      (let [letter (+ (rand-int 26) 97)]
        (char letter)))
    
    ;;; This should filter out words containing incorrectly-guessed
    ;;; letters.
    (defn negative-regex [game]
      (let [incorrectly-guessed-letters (.getIncorrectlyGuessedLetters game)]
        (if (empty? incorrectly-guessed-letters)
          "[a-z]"
          (format "[^%s]" (new String (char-array (.getIncorrectlyGuessedLetters game)))))))
    
    (defn letter->frequency [words]
      (loop [words words
             letter->frequency (hash-map)]
        (if words
          (let [word (first words)
                ;; `distinct' was here.
                letters word]
            ;; Also think about zipmap here.
            (recur (next words)
                   (reduce (fn [letter->frequency letter]
                             (assoc letter->frequency
                               letter
                               (+ (letter->frequency letter 0) 1)))
                           letter->frequency
                           letters)))
          letter->frequency)))
    
    (defn make-regex-strategy [dictionary]
      (let [dictionary (atom dictionary)]
        (reify
          GuessingStrategy
          (nextGuess [_ game]
            (prof
             :next-guess
             (let [guessed-so-far (.toLowerCase (.getGuessedSoFar game))
                   negative-regex (negative-regex game)
                   filtering-regex (format "^%s$"(replace-str "-" negative-regex guessed-so-far))
                   filtering-pattern (re-pattern filtering-regex)]
               ;; Would in-place filtering work here?
               (swap!
                dictionary
                (fn [dictionary]
                  (prof :filter (filter (fn [word] (re-seq filtering-pattern word)) dictionary))))
               ;; Some heuristic here for guessing.
               (if (= (count @dictionary) 1)
                 (new GuessWord (first @dictionary))
                 ;; Bidirectional map?
                 (let [letter->frequency
                       (prof
                        :letter->frequency
                        (letter->frequency @dictionary))
                       guessed-letters
                       (map (fn [letter] (Character/toLowerCase letter)) (.getAllGuessedLetters game))
                       letter->frequency
                       (filter (fn [letter-frequency] (not (.contains guessed-letters (first letter-frequency)))) letter->frequency)
                       frequency->letter
                       (into (sorted-map-by >) (map-invert letter->frequency))]
                   (new GuessLetter (second (first frequency->letter)))))))))))
    
    ;;; Modify this to return time and value!
    (defmacro time-and-value [expr]
      `(let [start# (. System (nanoTime))
             ret# ~expr]
         [(/ (double (- (. System (nanoTime)) start#)) 1000000.0)
          ret#]))
    
    (def words
      '("comaker"
        "cumulative"
        "eruptive"
        "factual"
        "monadism"
        "mus"
        "nagging"
        "oses"
        "remembered"
        "spodumenes"
        "stereoisomers"
        "toxics"
        "trichromats"
        "triose"
        "uniformed"))
    
    (def reference-scores
      '(25 9 5 9 8 25 7 5 5 4 2 11 5 5 5))
    
    ;;; From the initial implementation.
    (def initial-scores
      '(25 8 7 9 10 25 7 5 5 4 3 10 5 7 13))
    
    (def initial-times
      '(914.201401
        624.136794
        457.645472
        1164.972
        854.34475
        98.380223
        950.42491
        132.590102
        387.955044
        403.945935
        246.297455
        548.420537
        603.323457
        276.769557
        640.156031))
    
    (defn average-vals [data key]
      (let [vals (map key (vals data))]
        (float (/ (reduce + vals) (count vals)))))
    
    (profile
     (let [arity->dictionary (make-arity->dictionary "words.txt")]
       (let [time-scores
             (map #(time-and-value
                    (run (new HangmanGame % 4)
                         (make-regex-strategy
                          (arity->dictionary (count %) '()))))
                  words)
             data
             (zipmap words (map (fn [initial-time
                                     reference-score
                                     [time score]]
                                  (let [delta-time (- time initial-time)
                                        delta-score (- score reference-score)]
                                    (let [percent-change-time (/ delta-time initial-time)
                                          percent-change-score (float (/ delta-score reference-score))]
                                      {:delta-time delta-time
                                       :percent-change-time percent-change-time
                                       :delta-score delta-score
                                       :percent-change-score percent-change-score})))
                                initial-times reference-scores time-scores))
             average-delta-time (average-vals data :delta-time)
             average-delta-score (average-vals data :delta-score)
             average-percent-change-time (average-vals data :percent-change-time)
             average-percent-change-score (average-vals data :percent-change-score)]
         {:data (into (sorted-map) data)
          :average-delta-time average-delta-time
          :average-percent-change-time average-percent-change-time
          :average-delta-score average-delta-score
          :average-percent-change-score average-percent-change-score})))
    
  #+END_SRC

  At some point, letter guesses don't give me any more information;
  unless I do a diff on the words left. When words are fewer than
  spaces, start guessing words?

  With manual profiling, =merge= is taking a shit-load of time; can we
  do something with =frequencies=? Can we fuck around with =transient=
  and =assoc!=?

  ([[http://java.dzone.com/clojuretips][#_]] is a removal macro like =#;=, by the way.)

  =jvisualvm= reveals that =clojure.lang.Util.hash()= is eating most
  of the processing time; thoughts for optimizing this?

  We have about 1.3M =Objects=, and 0.9M =TreeMap$Entries=; not to
  mention 0.5M =Strings=, 0.4M =Conses= and 0.16M =TreeMaps=. Hmm. Are
  these sets orthogonal?

  It looks like -- oh, shit: =distinct= is taking a lot of time;
  followed by: =hash=, =re-seq=, =filter=, =seq=, =re-matcher=,
  etc. So we do have a lot of regular expression time.

  Jesus: taking out =distinct= saves almost =%50=; it causes certain
  words to fail, on the other hand.

  Here are the results from removing =distinct= (time in
  milliseconds):

  #+BEGIN_SRC clojure
    {:average-delta-time -283.85242,
     :average-percent-change-time -0.4640968,
     :average-delta-score 1.4666667,
     :average-percent-change-score 0.296532}
  #+END_SRC

  With distinct, there is only a nominal (10%) difference in
  =:average-percent-change-score=:

  #+BEGIN_SRC clojure
    {:average-delta-time -79.898224,
     :average-percent-change-time -0.111338414,
     :average-delta-score 0.8666667,
     :average-percent-change-score 0.196532}
  #+END_SRC

  And with this latest change =re-seq= is now the biggest consumer
  (behind =swank.util.concurrent.mbox$receive.invoke=; is that an
  artifact of using swank, however?).

  Wow, by implementing the =arity->dictionary= preprocessing, I was
  able to get an average 70% efficiency:

  #+BEGIN_SRC clojure
    {:average-delta-time -353.4292,
     :average-percent-change-time -0.6696512,
     :average-delta-score 1.4666667,
     :average-percent-change-score 0.296532}
  #+END_SRC

  Too bad the score still sucks; besides =seq= now, [[http://pacific.mpi-cbg.de/javadoc/clojure/lang/Util.html#hash(java.lang.Object)][hash]] still
  consumes the most time: some kind of object comparison? Is
  [[http://clojuredocs.org/clojure_core/1.2.0/clojure.core/hash][clojure.core/hash]] the same as Java's =hashCode=? Possibly.

  We might want to try in-place filtering, since =seq= gets called a
  lot; we should do some manual profiling at this point, possibly,
  though, to determine where the lion's share of time is taken up.

  I want to give tries a try, though.

  One potential optimization is to do an initial letter-frequency
  histogram and merely subtract the words that get filtered; maybe
  you're quibbling over linear (i.e. micro) optizimations now, though.

** DONE Is it slower to access char-based hash-tables than int-based vectors?
   CLOSED: [2011-08-24 Wed 16:53]
   - CLOSING NOTE [2011-08-24 Wed 16:54] \\
     Vectors are an order of magnitude faster for large n; crossover is
     roughly 1000000: some of that may have been due to garbage
     collection, though. The int-based vector is only marginally more
     efficient than the char->int-based vector, though: 210
     ms. vs. 250 ms. for n = 1000000. There's probably some overhead
     associated with transienting a vector, etc.
     
     One advantage of using tries is that it can be totally
     vector-based with ints. (Don't even bother with arrays, by the
     way; vide infra.)
   #+BEGIN_SRC clojure
     ;;; Use doall, etc. instead of loop to force evaluation? Otherwise, we
     ;;; could just use (take (repeatedly #(rand-int 26))), etc.
     (let [n 1000000
           chars (loop [chars '()
                        n n]
                   (if (zero? n)
                     chars
                     (recur (cons (char (+ (rand-int 26) 97))
                                  chars)
                            (dec n))))
           ints (loop [ints '()
                       n n]
                  (if (zero? n)
                    ints
                    (recur (cons (rand-int 26) ints)
                           (dec n))))]
       ;; Char-based map
       (time
        (reduce (fn [histogram char]
                  (assoc histogram char (+ (histogram char 0) 1)))
                {}
                chars))
     
       ;; Char->int-based immutable vector
       (time
        (reduce (fn [vector char]
                  (let [i (- (int char) 97)]
                    (assoc vector i (+ (vector i) 1))))
                (vec (replicate 26 0))
                chars))
     
       ;; Char->int-based mutable vector
       (time
        (let [vector (transient (vec (replicate 26 0)))]
          (doseq [char chars]
            (let [i (- (int char) 97)]
              (assoc! vector i (+ (vector i) 1))))
          (persistent! vector)))
     
       ;; Int-based mutable vector
       (time
        (let [vector (transient (vec (replicate 26 0)))]
          (doseq [int ints]
            (assoc! vector int (+ (vector int) 1)))
          (persistent! vector)))
     
       ;; Really fucking slow
       (time
        (let [array (into-array Integer/TYPE (replicate 26 0))]
          (doseq [int ints]
            (aset-int array int (+ (aget array int) 1)))
          array)))
     
     ;; "Elapsed time: 897.958722 msecs"
     ;; "Elapsed time: 223.132804 msecs"
     ;; "Elapsed time: 245.723347 msecs"
     ;; "Elapsed time: 207.313954 msecs"
     ;; "Elapsed time: 13016.683057 msecs"
   #+END_SRC
* DONE Separate java and clojure dirs under src?
  CLOSED: [2011-08-18 Thu 01:59]
  Could accomplish this with =:source-path= and =:java-source-path=.
* DONE Strings as seqs
  CLOSED: [2011-08-17 Wed 23:51]
  #+BEGIN_SRC clojure
    ;;; Treating strings as seqs of characters.
    (assert
     (= (with-out-str
          (doseq [c "abc"] (println c)))
        "a\nb\nc\n"))
    
    ;;; Convert a string into a seq.
    (assert (= (seq "abc") '(\a \b \c)))
  #+END_SRC
* DONE Implement a trivial GuessingStrategy.
  CLOSED: [2011-08-17 Wed 23:52]
  #+BEGIN_SRC clojure
    (import '(com.factual.hangman
              GuessingStrategy
              GuessLetter
              GuessWord
              HangmanGame
              HangmanGame$Status))
    
    (defn can-keep-guessing?
      ([game] (= (.gameStatus game) HangmanGame$Status/KEEP_GUESSING)))
    
    ;;; We should abstract some of this into hangman.core.
    (defn run
      ([game strategy]
         ;; Can also just wrap this in with-open, since
         ;; assertCanKeepGuessing (in guessLetter and guessWord) will
         ;; throw an exception.
         (while (can-keep-guessing? game)
           (println game)
           (let [guess (.nextGuess strategy game)]
             (.makeGuess guess game)))
         (println game)
         (.currentScore game)))
    
    (defn random-letter []
      (let [letter (+ (rand-int 26) 97)]
        (char letter)))
    
    (defn make-random-strategy []
      (reify
        GuessingStrategy
        (nextGuess [_ game] (new GuessLetter (random-letter)))))
    
    (let [game (new HangmanGame "factual" 4)]
      (assert (= (.currentScore game) 0))
      (assert (= (.gameStatus game) HangmanGame$Status/KEEP_GUESSING))
      (run game (make-random-strategy)))
    
  #+END_SRC
* DONE Read in words, filter on regex.
  CLOSED: [2011-08-17 Wed 16:34]
  #+BEGIN_SRC clojure
    ;;; Roughly 50 msecs.
    (time
     (let [words
           (binding [*in* (reader "words.txt")]
             (loop [word (read-line) words '()]
               (if word
                 (recur (read-line) (cons word words))
                 words)))]
       (filter (partial re-find #"^[^a][^a][e]$") words)))
    
  #+END_SRC
* DONE Read in list of words.
  CLOSED: [2011-08-15 Mon 21:22]
  #+BEGIN_SRC clojure
    (use 'clojure.contrib.io)
    
    ;;; This is about twice as slow: it has to parse the line; it only
    ;;; gathers 53754 words, furthermore, whereas `wc words.txt' gives
    ;;; 173528.
    (time
     (with-in-reader
       "words.txt"
       (loop [word (read *in* false nil) words '()]
         (if word
           (recur (read *in* false nil) (cons word words))
           (count words)))))
    
    ;;; This gives 173529 (including the blank word).
    (time
     (with-open [dictionary (reader "words.txt")]
       (binding [*in* dictionary]
         (loop [word (read-line) words '()]
           (if word
             (recur (read-line) (cons word words))
             (count words))))))
    
    ;;; This is slightly slower than `loop'; gives the right result,
    ;;; though.
    (time (count (reduce (fn [x y] (cons y x)) '() (read-lines "words.txt"))))
    
  #+END_SRC
* DONE Regex
  CLOSED: [2011-08-15 Mon 19:51]
  #+BEGIN_SRC clojure
    (use 'clojure.contrib.str-utils)
    (re-find #"[a-z]+" "harro")
    (re-gsub #"h" "x" "harro")

  #+END_SRC
* DONE Clojure
  CLOSED: [2011-08-17 Wed 23:52]
  On [[http://blog.8thlight.com/articles/2010/12/6/clojure-libs-and-namespaces-require-use-import-and-ns][namespaces]]; let's relegate factual's hangman to
  src/com/factual/hangman, or should we create a separate repo
  containing a jar?

  [[http://p.hagelb.org/import-indent.html][Vectors vs. lists]] in namespaces (fuckers). [[http://www.vineetmanohar.com/2009/11/3-ways-to-run-java-main-from-maven/][Main from maven]], btw.;
  [[http://www.avajava.com/tutorials/lessons/how-do-i-specify-a-main-class-in-the-manifest-of-my-generated-jar-file.html][specifying in MANIFEST.MF]], too.

  On perusing the API:

  =defrecord= like SRFI-9; =comment= like =#;=; but cf =defstruct=;
  =delay=; =deliver=; =disj= like =delete=; also, =dissoc= for maps;
  =doall= for side effects; =doc= reads doc strings, also =^{doc:
  ...}=? cf. =dorun= vs. =doall=; =doseq= appears to be the analogy
  for =for-each=, though; =doall= and =dorun= force the lazy
  evaluation of a sequence, apparently; =dorun= ignores the
  side-effects on the sequence; cf. =unfold=? =doto=: =(doto (new
  java.util.HashMap) (.put "a" 1) (.put "b" 2))=; =do= like =begin=;
  =file-seq=; =fnext= like =cadr=; =for=: lazy list-comprehension;
  =force=; =format= strings (c-style); =gen-class=; =gen-interface=
  (requires compilation?); =gensym=, cool; =get=; =get-in=: "nested
  associative structure": alist? =hash-map=; =identity=; =if-let=;
  =if-not=; =inc=; =interleave= like =zip=; =intern=; =interpose=;
  =into= like =append=? =into-array=; =iterate=: x, (f x), (f (f x));
  =juxt= (ad-hoc, anyone?); =(keep f coll)=: ad-hoc identity filter on
  non-nil; =lazy-cat=; =lazy-seq=; =letfn=; =list*=; =macroexpand=;
  =make-hierarchy= relates to =derive=, =isa?=; =map-indexed=
  (sometimes useful); =memfn=: java method as first-class =fn= ([[http://blog.m.artins.net/clojure-integrating-with-java/][this]]
  is cool: =(map (memfn toUpperCase) ["a" "short" "message"])=);
  =memoize=; =merge= for maps; =meta= for metadata (=^{ ... }=?);
  =next= like =cdr=; =neg?= instead of =negative?= [analogical
  learning]; =nfirst= instead of =cdar=; =nnext= instead of =cddr=,
  etc.; =not-any?=; =not-every?=; =nth= instead of =list-ref=;
  =nthnext= for pairs? =partition=; =pmap= for parallel mapping; =pr=
  like =write= (=write= is intended for =read=, too, I think);
  =pr-str=; =print=, =println=: "human consumption"; =print-str=,
  =println-str=; =printf= (cf. =format=); =prn=: =pr= and =newline=;
  =pvalues=: parallel; =reductions=: intermediate values of
  reductions, too; =reify=: instantiate objects? =remove= also like
  =delete=? lazy; =repeat=; =repeatedly=: like =tabulate=; =replicate=
  like =make-list=; =reset!= like =set!=? =rseq=: reverse =seq=;
  =rsubseq=; =send= for agents; =seq=: seq on the collection; =seque=:
  queued seq; =sequence=: distinct from =seq=; =set=: distinct
  elements; =shuffle=; =some=: =(some #{:fred} coll)= (oh, yeah: sets
  can be used as predicates somehow to test for membership, right?);
  =sort=; =sorted-set=; =str=: to string; =subseq=; =subvec=; =take=;
  =take-last=; =trampoline=: mutual recursion without stack
  consumption; =with-bindings= and =with-bindings*= vs. =binding=
  (former two appear to be "[[http://groups.google.com/group/clojure/browse_thread/thread/c42a24c7b927c2b5][low level]]")?
* DONE Leiningen
  CLOSED: [2011-08-17 Wed 23:52]
  [[http://alexott.net/en/clojure/ClojureLein.html][Pretty good intro]]; workflow:

  #+BEGIN_QUOTE
  - you create a project (=lein new=), define dependencies on external
    libraries and download them with =lein deps= command (you need to
    run it after each change of dependencies);
  - you write your code, periodically running =lein compile=, =lein
    test=, and may be using =lein repl=, =lein swank= or =lein
    nailgun= (depending on your personal preferences) for interactive
    development;
  - if you develop a library, that you plan to use in other projects,
    then you can install it into Maven's local repository with =lein
    install= command, or you can upload it to Clojars (with scp, as
    suggested in documentation, or by using the lein-clojars plugin);
  - if you develop a program for end-user, then you can pack your code
    into package with =lein jar= command (only your code, without
    dependencies), or with =lein uberjar=, with all dependencies
    included into package — it's much easier to distribute such
    packages to end users.
  #+END_QUOTE
* CANCELED call-cc
  CLOSED: [2011-08-22 Mon 19:34]
  Unlike the scheme example (which depends heavily on mutation) runs
  to the very end of the list.

  #+BEGIN_SRC clojure
    (use 'clojure.contrib.monads)
    (run-cont (call-cc (fn [k] (k 2))))
    (defn generate-one-element-at-a-time [list]
      (defn control-state [return]
        (doseq [element list]
          (def return
            (call-cc
             (fn [resume-here]
               (def control-state resume-here)
               (return element)))))
        (return 'fuck-this-shit))
      (defn generator []
        (call-cc control-state))
      generator)
    
    (def generate-digit (generate-one-element-at-a-time '(0 1 2)))
    (run-cont (generate-digit))
    
  #+END_SRC

  See [[http://repository.readscheme.org/ftp/papers/PLoP2001_dferguson0_1.pdf][call/cc patterns]]; same here:

  #+BEGIN_SRC clojure
    (use 'clojure.contrib.monads)
    
    (defn generate-one-element-at-a-time [list]
      (def control-state
        (atom
         (fn [return]
           (let [return (atom return)]
             (doseq [element list]
               (swap!
                return
                (fn [return]
                  (call-cc
                   (fn [resume-here]
                     (swap! control-state
                            (fn [control-state] resume-here))
                     (return element)))))))
           (return 'fuck-this-shit))))
      (defn generator []
        (call-cc @control-state))
      generator)
    
    (def generate-digit (generate-one-element-at-a-time '(0 1 2)))
    (run-cont (generate-digit))
    
  #+END_SRC

  We need a different pattern (see continuation monads in Haskell)
  that doesn't rely on mutating state variables?

  From [[http://en.wikibooks.org/wiki/Haskell/Continuation_passing_style][CPS in Haskell]]:

  #+BEGIN_SRC clojure
    (use 'clojure.contrib.monads
         'clojure.contrib.math)
    
    ((fn [n] (run-cont (call-cc (fn [k] (k (* n n)))))) 2)
    
    ;; (def continuation nil)
    
    ;; (defn incrementer [x]
    ;;   (fn [k]
    ;;     (loop [x (+ x 1)]
    ;;       (def continuation k)
    ;;       (recur x))))
    
    ;; (def i (incrementer 1))
    ;; (run-cont i)
    
    (def continuation nil)
    
    (run-cont
     (domonad cont-m
              [x (m-result 1)
               y (call-cc (fn [c] (def continuation c) (c 2)))]
              (+ x y)))
    
    (run-cont (continuation 5))
    (run-cont (continuation 42))
    (run-cont (continuation -1))
  #+END_SRC

  #+BEGIN_SRC scheme
    ;; [LISTOF X] -> ( -> X u 'you-fell-off-the-end)
    (define (generate-one-element-at-a-time lst)
      
      ;; Hand the next item from a-list to "return" or an end-of-list marker
      (define (control-state return)
        (for-each 
         (lambda (element)
           (set! return (call-with-current-continuation
                         (lambda (resume-here)
                           ;; Grab the current continuation
                           (set! control-state resume-here)
                           (return element)))))
         lst)
        (return 'you-fell-off-the-end))
      
      ;; (-> X u 'you-fell-off-the-end)
      ;; This is the actual generator, producing one item from a-list at a time
      (define (generator)
        (call-with-current-continuation control-state)) 
      
      ;; Return the generator 
      generator)
    
    (define generate-digit
      (generate-one-element-at-a-time '(0 1 2 3 4 5 6 7 8 9)))
    
    (generate-digit)
    (generate-digit)
    
  #+END_SRC

* CANCELED Continuations
  CLOSED: [2011-08-18 Thu 01:53]
  Someone did [[https://github.com/swannodette/delimc][delimited continuations]].

  #+BEGIN_SRC clojure
    (use 'delimc.core)
    (assert (= (+ 1 (reset (+ 2 3))) 6))
    (assert (= (+ 1 (reset (+ 2 (shift k 3)))) 4))
    (assert (= (+ 1 (reset (+ 2 (shift k (+ 3 (k 4)))))) 10))
    
  #+END_SRC

  Given the pre-defined interface, though, I'm not sure how we'd fold
  it in: a global continuation, of course, which is mutated by
  =reset!=. Is that principally different than mutating the dictionary state
  of the =GuessingStrategy=?
* CANCELED Graph
  CLOSED: [2011-08-17 Wed 23:53]
  - CLOSING NOTE [2011-08-17 Wed 23:53] \\
    We're not going to go with quasi-Markov-chains now; regex, then
    optimization with [[regex-then-tries][tries]], if necessary.
  Graph based on input words, where the weight is the number of times
  a letter follows another (or precedes?) (this is Markov, isn't it?);
  and an frequency table of letters to seed the guesses.

  Then we do a Dijkstra weighted--shortest-path and greedy
  optimization?

  At some point we need to do a calculation, though, don't we: given
  that we have so many guesses left, etc., what's the cost of guessing
  a word vs. guessing a letter?

  We have neither secretWord nor maxWrongGuesses, though.

  Going with the Dijkstra analogy, we remove edges for wrong guesses;
  each edge represents a potential digram. (Could this be improved
  with n-grams, etc.?)

  1. Get the game;
  2. determine how many letters;
  3. take appropriate words;
  4. build a frequency table for initial guesses (randomizing and
     eliminating ties?);
  5. build a weighted digram graph;
  6. guess correctly: prune the graph (all paths at index {a1, ..., an}
     converge on a) (question is: do we distinguish between letter
     e.g. `a' at positions e.g. 1 and 3? If we permit cycles, the
     links have lower quality; how do we reference index i in the
     graph, though? We do have some starting nodes and perhaps even a
     forest (shit: one graph for each starting letter));
  7. guess incorrectly: prune the graph (no such letter exists).

  We're dealing with a sort of tree here, aren't we, whose leaves are
  words? In other words, given a sequence of correct guesses; it
  should be unambiguous that it signifies a given word. Better yet: we
  should be able to determine the set of signifiable words; we could
  have some heuristic: possible words vs. =maxWrongGuesses=, but I
  don't think we have =maxWrongGuesses=.

  Not true! We have =numWrongGuessesRemaning()= and
  =getMaxWrongGuesses()=. =getSecretWordLength()=: nice! That's what I
  was looking for.

  If =numWrongGuessesRemaning= > $|signifiableWords|$, then we can
  clearly guess all the $\{signifiableWords\}$ one-by-one.

  Thing we're missing is: how do we go from an incompletely determined
  graph to possible words? That's why I suspect we're dealing with,
  not a tree per se, but a graph with terminal vertices; graph if we
  converge on letters: i.e. $\text{c}_1 \to \text{a}_2 \to
  \text{b}_3$, $\text{f}_1 \to \text{a}_2 \to \text{b}_3$; as opposed
  to: $\text{c}_{1_1} \to \text{a}_{2_1} \to \text{b}_{3_1}$,
  $\text{f}_{1_1} \to \text{a}_{2_2} \to \text{b}_{3_2}$. Convergence
  seems to make sense from a space point-of-view, as well as
  determining convergent paths.

  Create a special root node: one artificial graph (graph with
  terminal vertices, not a tree).

  #+BEGIN_SRC makefile :tangle graph.mk :shebang #!/usr/bin/unexpand -t 4
    CLASSPATH := .:$(shell echo lib/*.jar | tr ' ' ':')
    SOURCE := $(wildcard *.java) $(wildcard *.scm)
    OBJECTS := $(patsubst %.java,%.class,$(wildcard *.java)) \
        $(patsubst %.scm,%.scc,$(wildcard *.scm))
    
    .PHONY: test
    
    %.class : %.java
        javac -classpath $(CLASSPATH) $<
    
    %.scc : %.scm
        sisc -e "(compile-file \"$<\" \"$@\")"
    
    test: all
        java -classpath $(CLASSPATH) Hangman words.txt factual
    
    all: $(OBJECTS)
    
  #+END_SRC

  #+BEGIN_SRC java :tangle Hangman.java
    import java.lang.reflect.Array;
    import java.util.Arrays;
    import java.util.Queue;
    import java.io.IOException;
    
    import sisc.interpreter.Interpreter;
    import sisc.interpreter.Context;
    import sisc.interpreter.SchemeCaller;
    import sisc.interpreter.SchemeException;
    import sisc.data.EmptyList;
    import sisc.data.Symbol;
    import sisc.data.Pair;
    import sisc.data.Value;
    import sisc.data.SchemeString;
    import sisc.util.Util;
    import sisc.modules.s2j.JavaObject;
    
    public class Hangman {
        public static final int maxWrongGuesses = 5;
    
        public static int run(HangmanGame game, GuessingStrategy strategy) {
            return 0;
        }
    
        public static void main(final String[] args) throws SchemeException, IOException {
            String dictionary = args[0];
            String[] words = Arrays.copyOfRange(args, 1, args.length);
            for (String word: words) {
                HangmanGame game = new HangmanGame(word, maxWrongGuesses);
                GuessingStrategy strategy = new GraphGuessingStrategy(dictionary, game);
                run(game, strategy);
            }
        }
    }
    
  #+END_SRC

  #+BEGIN_SRC scheme :tangle hangman.scm
    (import s2j)
    
    (define-syntax debug
      (syntax-rules ()
        ((_ x ...)
         (begin
           (write `((x ,x) ...))
           (newline)))))
    
    (define-java-classes
      <guessing-strategy>
      <hangman-game>
      <hangman>)
    
    (define-java-proxy
      (guessing-strategy dictionary game)
      (<guessing-strategy>)
      (define x 2)
      #;
      (define (guessing-strategy dictionary game)
        (set! x 3)
        (debug x))
      (define (next-guess game)
        #;(game get-secret-word-length)
        2))
    
    ;; (debug (java-proxy-class <hangman-game>))
    ;; (debug (java-class-name <hangman-game>)
    ;;        (java-method-name guessing-strategy))
    
    ;; (define-generic-java-methods next-guess)
    ;; (define-generic-java-field-accessors :next-guess)
    
    (define (make-dictionary dictionary)
      (with-input-from-file
          dictionary
        (lambda ()
          (unfold ))))
    
    (define (init strategy dictionary game)
      (debug 2)
      2)
    
    (define (main argv)
      (let ((dictionary (car argv))
            (words (cdr argv)))
        (debug 2 dictionary words)
        ;; (debug (java-new <java.util.linked-hash-set> (->jint 100)))
        ;; (debug (java-new <hangman-game> (->jstring "factual") (->jint 4)))
        (let* ((game (java-new <hangman-game> (->jstring "factual") (->jint 4)))
               (strategy (guessing-strategy dictionary game)))
          strategy)
        #;
        (let ((game (java-new <hangman-game> (->jstring "factual") (->jint 4))))
          (debug game))
    
        ;; (java-new <hangman-game> (->jstring "oeunth") (->jint 5))
        ;; (java-new <hangman-game> "oeuhnt" 5)
        #;
        (with-input-from-file
        dictionary
        (lambda ()
        (let next-word ((word (read)))
        (if (eof-object? word)
        0
        (begin
        (debug word)
        (next-word (read)))))))))
    
  #+END_SRC

  I'm going to assume that the program takes a dictionary file and
  list of words; and that I have to program the glue.

  How is this going to work with initialization, etc.? For clarity,
  should we implement the interface in Java, there to defer to Scheme?

  No, let's do it in Scheme. (Risky.)
  
  We'll define the main run-loop in Java (=run(game, strategy)=); the
  creation of the strategy, however, will defer to Scheme (=new
  GraphStrategy(dictionary)=).

  #+BEGIN_SRC java :tangle GraphGuessingStrategy.java
    import java.io.IOException;
    
    import sisc.interpreter.Interpreter;
    import sisc.interpreter.Context;
    import sisc.interpreter.SchemeCaller;
    import sisc.interpreter.SchemeException;
    import sisc.data.EmptyList;
    import sisc.data.Symbol;
    import sisc.data.Pair;
    import sisc.data.Value;
    import sisc.data.SchemeString;
    import sisc.util.Util;
    import sisc.modules.s2j.JavaObject;
    
    public class GraphGuessingStrategy implements GuessingStrategy {
        
    
        public Guess nextGuess(HangmanGame game) {
            return new Guess() {
                public void makeGuess(HangmanGame game) {
                }
            };
        }
    
        // Let this take, instead, a dictionary-graph.
        public GraphGuessingStrategy(final String dictionary, final HangmanGame game) throws SchemeException {
            Context.execute(new SchemeCaller() {
                    public Object execute(Interpreter interpreter) throws SchemeException {
                        interpreter.loadSourceFiles(new String[] { "hangman.scc" });
                        try {
                            interpreter.eval(Util.proc(interpreter.eval("init")),
                                             new Value[] {
                                                 new JavaObject(this),
                                                 new SchemeString(dictionary),
                                                 new JavaObject(game)
                                             });
                        } catch (Throwable e) {
                            e.printStackTrace();
                        };
                        return true;
                    }
                });
        }
    }
    
  #+END_SRC

  Let's initialize a "strategy-factory" with the dictionary that
  creates a graph for each class of words with letters $2 .. n$. No,
  fuck that: initialize the =GraphGuessingStrategy= with the
  dictionary-graph.

  How do we share the graphs, though? Either it's some sort of opaque
  scheme-object corresponding to, say, a hash-table; or it's some Java
  object (say, =java.util.HashTable=) with which we have to interact
  from Scheme. What are the performance characteristics of both
  scenarios?

  For convenience, I'd rather have Scheme objects to pass around (a
  =Value=, essentially).

  (Oh, check it out: they have a [[http://sisc-scheme.org/manual/javadoc/sisc/modules/hashtable/Hashtable.html][Hashtable]] interface.)

  #+BEGIN_SRC scheme :tangle unfold-file.scm
    (require-library 'sisc/libs/srfi/srfi-1)
    (import srfi-1)
    (import s2j)
    ;; (load "irregex-0.8.1/irregex.scm")
    ;; (require-extension (lib scsh-regexp/scsh-regexp))
    ;; (import scsh-regexp/scsh-regexp)
    
    (define-java-classes
      <java.lang.string>
      <java.util.regex.pattern>)
    
    (define-generic-java-methods
      matches)
    
    (let ((dictionary
           (with-input-from-file
               "words.txt"
             (lambda ()
               (let next-word ((word (read))
                               (dictionary '()))
                 (if (eof-object? word)
                     dictionary
                     (next-word
                      (read)
                      (cons (->jstring word) dictionary))))))))
      #;
      (let ((pattern (->jstring "...")))
        (filter (lambda (word)
                  (->boolean (matches word pattern)))
                dictionary))
      ;; Adds 6 sec.
      #;
      (let ((pattern (->jstring ".......")))
        (filter (lambda (word)
                  (->boolean (matches word pattern)))
                dictionary))
      ;; Adds 6 sec.
      #;
      (let ((pattern (->jstring "...a...")))
        (filter (lambda (word)
                  (->boolean (matches word pattern)))
                dictionary))
      ;; Adds 4 sec; should we bother compiling the regex?
      (let ((pattern (->jstring "...a...")))
        (filter (lambda (word)
                  (->boolean (matches word pattern)))
                (let ((pattern (->jstring ".......")))
                  (filter (lambda (word)
                            (->boolean (matches word pattern)))
                          dictionary))))
      2
      #;
      (for-each (lambda (word)
      'harro
      ;; (irregex-match '(w/nocase ".*") word)
      ;; (matches (->jstring word) (->jstring ".?"))
      #;
      (let ((word (java-new <java.lang.string> (->jstring word))))
      (matches (java-null <java.util.regex.pattern>) word (->jstring ".?"))))
      dictionary))
    
  #+END_SRC

  The issue is: either we have to deal with Scheme strings, Java
  strings or some kind of conversion; the conversion is relatively
  slow (have to do so for e.g. =java.lang.String.match()=); and
  irregex doesn't work (otherwise we could stick with Scheme strings).

  Can we do slow regex for now, and replace it with some e.g. trie
  algo later?

  I just realized that the chaining doesn't matter; we're only dealing
  with frequency, I believe. Could roughly:

  1. Filter the current dictionary based on =getGuessedSoFar()= (this
     could be a string->regex substitution for now and would thus
     carry an O(n * O(regex)) penalty; some sort of e.g. trie-based
     search might get us log-linear).
  2. Is the $|current dictionary| < numWrongGuessesRemaning$? Guess!
  3. Build a frequency list of letters.
  4. Guess one of the most frequent letters; remove it.
  5. Wrong?
     1. Guess again.
  6. Right?
     1. Goto 1.

  Just met Aaron at the Hacker News meetup: drop Scheme, adopt
  Clojure.

  [[http://stackoverflow.com/questions/6339473/python-regex-hangman-algorithm][This]] is subtle, by the way: regexps are susceptible to false
  positives. From the same page:

  #+BEGIN_QUOTE
  As far as determining the next character to guess, you probably
  don't want to select the most frequent character. Instead, you want
  to select the character that comes closest to being in 50% of words,
  meaning you eliminate the most possibilities either way.
  #+END_QUOTE

  Also:

  #+BEGIN_QUOTE
  Your regex should look more like this: =^e[^e][^e][^e][^e]e[^e]$=.
  #+END_QUOTE

  Good call.

  [[http://qs343.pair.com/~monkperl/index.pl?node_id=780414][Frequency and density]]; also: probability that letter is in a given
  position!

  #+BEGIN_QUOTE
  From this, you can easily duplicate my algorithm by determining
  which letter to choose based on which word appears in the most
  candidate words (highest probability of being right) and break ties
  by using the highest overall density.
  #+END_QUOTE

  #+BEGIN_QUOTE
  First, a 1 size fits all algorithm that weighs various different
  pieces of data to produce an overall score. The advantage here is
  that you should be able to tune these weights without touching code
  to produce the best results for a given "dictionary". The second
  approach would be to have multiple algorithms already fine tuned for
  a given set of conditions and dispatch to that algorithm based on
  initial conditions. For instance, it may be ok to take a risk of
  guessing wrong if the remaining wrong guesses is still high or
  perhaps utilize the "simple" algorithm already presented unless the
  probabilities and candidates remaining (if right or wrong) is beyond
  some threshold.
  #+END_QUOTE

  The Markov chains come up again:

  #+BEGIN_QUOTE
  An higher order approach would be to look at probabilities for
  chains of letters. E. g. in English the chance for an 'u' should be
  higher than average after a 'q'.
  #+END_QUOTE

  He's right, actually: looking at mere frequency represents a loss of
  data (`q' followed by `u', for instance, is not handled). Let's
  revisit the Markov idea.

  On the other hand, maybe I was right:

  #+BEGIN_QUOTE
  If 'q' is guessed and is correct, then the next time around, 'u' will
  already jump to the top of the list of letters with the most
  probability to be correct. You don't need to know any special
  knowledge about letter frequency (alone or in chains) to get this benefit.
  #+END_QUOTE

  [[http://qs343.pair.com/~monkperl/index.pl?node_id=779296][Original post]]; this guy is suggesting some kind of binary search:

  #+BEGIN_QUOTE
  At least in your example, a letter never appears in more than half
  of the candidates, so the most frequent letter and the
  closest-to-half-half letter coincide. But imagine if you found out
  that the actual word contained Q. Then for sure the next most common
  letter will be a U; but guessing U and getting it right will not
  give you much new information. Aiming for half-half lets your
  correct and incorrect guesses both contribute information.
  #+END_QUOTE  

  Position:

  #+BEGIN_QUOTE
  My strategy is optimized not to guess wrong. After only 5 guesses (2
  right and 3 wrong) it had narrowed the search space down to exactly
  1 word. This is because I prune by position not just by presence of
  letter so even successful guesses on a popular letter can still
  effectively decrease the search space. Your approach would be
  improved with this strategy as well. I don't think the opportunities
  stop there.
  #+END_QUOTE

  #+BEGIN_QUOTE
  Note that there might be other ways to score each possible
  next-letter guess. Number of non-empty buckets comes to mind as an
  "average case" measure (to be maximized). Again, this is all
  assuming we're minimizing the total number of guesses. That way, all
  of the possible outcomes are (i.e., the guessed letter appears or
  doesn't appear in the word) are treated the same. To minimize the
  number of wrong guesses, you have to treat the "doesn't appear in
  the word" outcome differently and weight things in some better way.
  #+END_QUOTE

  Regex should account for position, shouldn't it?

  #+BEGIN_QUOTE
  Now let's assume we were going with 'eclectic'. There were 9,638
  words in my dictionary with a length of 8. The letter that appeared
  in the most of those words was the letter 'e' at 6,666. Note that I
  didn't count total occurrences of 'e' but only 1 per word. This
  equated to 69%. Now what I set out to do was map the different ways
  the letter 'e' appeared across those 6,666 words. In the word
  'eclectic' it appears in positions 1 and 4 where as in 'envelope' it
  appears in positions 1, 4 and 8. After guessing and seeing which
  positions were filled in, I could even eliminate words with letter
  'e' even if they shared the common position because they didn't
  share all positions. That last part (all positions) was the piece I
  hadn't considered in my previous exercise. So here is the mind
  blowing part. The most common set of positions across the 6,666
  words with the letter 'e' still had less than 1,600 possible
  words. This means that by selecting the letter 'e' (69% chance of
  being right) I will reduce the candidate list from 9,638 to less
  than 1,600 (and probably a lot further). It seemed pointless then to
  come up with some weights for determining letter based on
  probability of being correct and degree to which the candidate list
  is reduced because the "dumb" method was still doing a superb job.

  I do have one last final revision to make. I choose the letter that
  appears in the most candidate words but I don't break
  ties. Currently it is the result of sort. I plan to add total count
  as a secondary tie breaking condition to see if that improves
  results. I should post something later today.
  #+END_QUOTE
* Threading and de-structuring
  [[http://java.dzone.com/clojuretips][Interesting tips]]; as well as: =assoc=-reduction.
* Description
** The Goal

   Write a program that plays the Hangman game. A description of the
   game can be found at http://en.wikipedia.org/wiki/Hangman_(game).

** The Problem

   Your goal is to use a provided API to play Hangman efficiently. You need
   to guess a word using as few guesses as possible, and make no more than
   maxWrongGuesses incorrect guesses. You are writing the letter/word
   guessing strategy.

   We would like you to use the provided Java APIs and to write your solution
   in Java. However, if you don't know Java or are not comfortable with it,
   please feel free to use other reasonably mainstream programming languages
   (C++, Python, Ruby, Scala, etc.). If you do use another language, please
   make sure that your program can accept a dictionary file and a list of
   test words (that are in the dictionary). The output of the program should
   be a score for each test word and the average score across all test words.
   Also, if you don't use Java, please provide build instructions if they are
   not straightforward.

   Your score for a word will be:

   #+BEGIN_QUOTE
   # letter guesses + # number of incorrect word guesses if
   you guessed the word right before making maxWrongGuesses incorrect
   guesses
   #+END_QUOTE

   or

   #+BEGIN_QUOTE
   25 if you lost the game before guessing the word correctly.
   #+END_QUOTE

   You will need to write an implementation of the GuessingStrategy interface
   and some code to use your GuessingStrategy on a HangmanGame instance.

   The pseudocode to run your strategy for a HangmanGame is:

   #+BEGIN_SRC java
     // runs your strategy for the given game, then returns the score
     public int run(HangmanGame game, GuessingStrategy strategy) {
         while (game has not been won or lost) {
             ask the strategy for the next guess apply the next guess to
             the game
         }
         return game.score();
     }
   #+END_SRC

     A trivial strategy might be to guess 'A', then 'B', then 'C', etc. until
     you've guessed every letter in the word (this will work great for "cab"!)
     or you've lost.

     Every word you encounter will be a word from the words.txt file.

** Example

   For example, let's say the word is FACTUAL.

   Here is what a series of calls might look like:

   #+BEGIN_SRC java
     HangmanGame game = new HangmanGame("factual", 4); // secret word is
                                                       // factual, 4 wrong
                                                       // guesses are
                                                       // allowed
     System.out.println(game);
     new GuessLetter('a').makeGuess(game);
     System.out.println(game);
     new GuessWord("natural").makeGuess(game);
     System.out.println(game);
     new GuessLetter('x').makeGuess(game);
     System.out.println(game);
     new GuessLetter('u').makeGuess(game);
     System.out.println(game);
     new GuessLetter('l').makeGuess(game);
     System.out.println(game);
     new GuessWord("factual").makeGuess(game);
     System.out.println(game);
   #+END_SRC

   The output would be:

   #+BEGIN_EXAMPLE
     -------; score=0; status=KEEP_GUESSING
     -A---A-; score=1; status=KEEP_GUESSING
     -A---A-; score=2; status=KEEP_GUESSING
     -A---A-; score=3; status=KEEP_GUESSING
     -A--UA-; score=4; status=KEEP_GUESSING
     -A--UAL; score=5; status=KEEP_GUESSING
     FACTUAL; score=5; status=GAME_WON
   #+END_EXAMPLE

   =game.score()= will be 5 in this case since there were 4 letter
   guesses and 1 incorrect word guess made.

** Sample Data
   As a baseline, here are scores for a reasonably good guessing strategy
   against a set of 15 random words. Your strategy will likely be better for
   some of the words and worse for other words, but the average score/word
   should be in the same ballpark.

   #+BEGIN_QUOTE
   #+BEGIN_EXAMPLE
   COMAKER = 25 (was not able to guess the word before making more than 5
   mistakes)
   CUMULATE = 9
   ERUPTIVE = 5
   FACTUAL = 9
   MONADISM = 8
   MUS = 25 (was not able to guess the word before making more than 5
   mistakes)
   NAGGING = 7
   OSES = 5
   REMEMBERED = 5
   SPODUMENES = 4
   STEREOISOMERS = 2
   TOXICS = 11
   TRICHROMATS = 5
   TRIOSE = 5
   UNIFORMED = 5
   #+END_EXAMPLE
   #+END_QUOTE

** Resources

   You should have been provided with a zip file with source code and
   a dictionary file to get you started. If you were not sent this zip
   file, or you have any questions about the contents, please let us
   know right away.

   The resources contain a dictionary file called words.txt. You can
   assume all words that your program will be tested with come from
   this dictionary file.

** Judging

   Your solution will be graded on the following criteria

     - code quality, readability, and design
     - performance (speed/memory footprint). We are more concerned with
       average time per game, so expensive one-time initialization is okay
       (as long as it's not too egregious)
     - total score for 100 random words, compared to the total score of
       several reference implementations for the same 100 words (max wrong
       guesses is typically set to 5)


